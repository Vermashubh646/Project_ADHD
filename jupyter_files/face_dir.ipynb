{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Face Mesh.\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How a Baseline Calibration Helps\n",
    "\n",
    "Personal Variation:\n",
    "Every individual has unique facial geometry and a natural resting pose. The generic model might, for example, consider a slight tilt as \"not focused,\" even though that tilt is normal for that person.\n",
    "\n",
    "Baseline Calibration:\n",
    "By capturing, say, the first 30 frames when you assume the user is looking directly at the screen, you can compute the average Euler angles (pitch, yaw, and roll) for that user. This average serves as their personal baseline.\n",
    "\n",
    "Relative Comparison:\n",
    "For subsequent frames, instead of checking if each angle is within an absolute ±10° range, you compare the current angles to the baseline. If the differences exceed your thresholds, then you can conclude the head position has changed (e.g., the user might be distracted or tilted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start capturing video from the webcam.\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "# Calibration Phase: Capture the first 30 frames for baseline.\n",
    "calibration_frames=30\n",
    "calibration_angles=[]\n",
    "\n",
    "\n",
    "while len(calibration_angles) < calibration_frames:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Flip the frame for a natural selfie-view and convert color space.\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    h, w, _ = frame.shape\n",
    "\n",
    "    # Process the frame with MediaPipe Face Mesh.\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "    \n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            # Extract 2D image points using specific landmark indices.\n",
    "            # Recommended indices for head pose estimation:\n",
    "            # Nose tip: 1, Chin: 152, Left eye left corner: 33,\n",
    "            # Right eye right corner: 263, Left mouth corner: 61, Right mouth corner: 291.\n",
    "            image_points = np.array([\n",
    "                (face_landmarks.landmark[1].x * w, face_landmarks.landmark[1].y * h),    # Nose tip\n",
    "                (face_landmarks.landmark[152].x * w, face_landmarks.landmark[152].y * h),  # Chin\n",
    "                (face_landmarks.landmark[33].x * w, face_landmarks.landmark[33].y * h),    # Left eye left corner\n",
    "                (face_landmarks.landmark[263].x * w, face_landmarks.landmark[263].y * h),  # Right eye right corner\n",
    "                (face_landmarks.landmark[61].x * w, face_landmarks.landmark[61].y * h),    # Left mouth corner\n",
    "                (face_landmarks.landmark[291].x * w, face_landmarks.landmark[291].y * h)   # Right mouth corner\n",
    "            ], dtype=\"double\")\n",
    "            \n",
    "            # Define corresponding 3D model points in a generic face coordinate system.\n",
    "            model_points = np.array([\n",
    "                (0.0, 0.0, 0.0),         # Nose tip\n",
    "                (0.0, -63.6, -12.5),     # Chin\n",
    "                (-43.3, 32.7, -26.0),    # Left eye left corner\n",
    "                (43.3, 32.7, -26.0),     # Right eye right corner\n",
    "                (-28.9, -28.9, -24.1),   # Left mouth corner\n",
    "                (28.9, -28.9, -24.1)     # Right mouth corner\n",
    "            ])\n",
    "            \n",
    "            # Set up the camera matrix using the frame dimensions.\n",
    "            focal_length = w\n",
    "            center = (w / 2, h / 2)\n",
    "            camera_matrix = np.array([\n",
    "                [focal_length, 0, center[0]],\n",
    "                [0, focal_length, center[1]],\n",
    "                [0, 0, 1]\n",
    "            ], dtype=\"double\")\n",
    "            dist_coeffs = np.zeros((4, 1))  # Assuming no lens distortion\n",
    "            \n",
    "            # Solve for the head pose using solvePnP.\n",
    "            success, rotation_vector, translation_vector = cv2.solvePnP(\n",
    "                model_points, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_ITERATIVE\n",
    "            )\n",
    "            \n",
    "            if success:\n",
    "                # Convert rotation vector to rotation matrix.\n",
    "                rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
    "                # Decompose the rotation matrix to get Euler angles.\n",
    "                retval, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rotation_matrix)\n",
    "                # retval contains the Euler angles (pitch, yaw, roll) in degrees.\n",
    "                euler_angles = np.array(retval).flatten()  # Flatten to ensure a 1D array.\n",
    "                pitch = float(euler_angles[0])\n",
    "                yaw   = float(euler_angles[1])\n",
    "                roll  = float(euler_angles[2])\n",
    "\n",
    "                calibration_angles.append((pitch, yaw, roll))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Compute average baseline angles.\n",
    "baseline_pitch = np.mean([angle[0] for angle in calibration_angles])\n",
    "baseline_yaw = np.mean([angle[1] for angle in calibration_angles])\n",
    "baseline_roll = np.mean([angle[2] for angle in calibration_angles])\n",
    "\n",
    "\n",
    "# Monitoring Phase:\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Flip the frame for a natural selfie-view and convert color space.\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    h, w, _ = frame.shape\n",
    "\n",
    "    # Process the frame with MediaPipe Face Mesh.\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "    \n",
    "    head_facing_forward = False  # Default state\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            # Extract 2D image points using specific landmark indices.\n",
    "            # Recommended indices for head pose estimation:\n",
    "            # Nose tip: 1, Chin: 152, Left eye left corner: 33,\n",
    "            # Right eye right corner: 263, Left mouth corner: 61, Right mouth corner: 291.\n",
    "            image_points = np.array([\n",
    "                (face_landmarks.landmark[1].x * w, face_landmarks.landmark[1].y * h),    # Nose tip\n",
    "                (face_landmarks.landmark[152].x * w, face_landmarks.landmark[152].y * h),  # Chin\n",
    "                (face_landmarks.landmark[33].x * w, face_landmarks.landmark[33].y * h),    # Left eye left corner\n",
    "                (face_landmarks.landmark[263].x * w, face_landmarks.landmark[263].y * h),  # Right eye right corner\n",
    "                (face_landmarks.landmark[61].x * w, face_landmarks.landmark[61].y * h),    # Left mouth corner\n",
    "                (face_landmarks.landmark[291].x * w, face_landmarks.landmark[291].y * h)   # Right mouth corner\n",
    "            ], dtype=\"double\")\n",
    "            \n",
    "            # Define corresponding 3D model points in a generic face coordinate system.\n",
    "            model_points = np.array([\n",
    "                (0.0, 0.0, 0.0),         # Nose tip\n",
    "                (0.0, -63.6, -12.5),     # Chin\n",
    "                (-43.3, 32.7, -26.0),    # Left eye left corner\n",
    "                (43.3, 32.7, -26.0),     # Right eye right corner\n",
    "                (-28.9, -28.9, -24.1),   # Left mouth corner\n",
    "                (28.9, -28.9, -24.1)     # Right mouth corner\n",
    "            ])\n",
    "            \n",
    "            # Set up the camera matrix using the frame dimensions.\n",
    "            focal_length = w\n",
    "            center = (w / 2, h / 2)\n",
    "            camera_matrix = np.array([\n",
    "                [focal_length, 0, center[0]],\n",
    "                [0, focal_length, center[1]],\n",
    "                [0, 0, 1]\n",
    "            ], dtype=\"double\")\n",
    "            dist_coeffs = np.zeros((4, 1))  # Assuming no lens distortion\n",
    "            \n",
    "            # Solve for the head pose using solvePnP.\n",
    "            success, rotation_vector, translation_vector = cv2.solvePnP(\n",
    "                model_points, image_points, camera_matrix, dist_coeffs, flags=cv2.SOLVEPNP_ITERATIVE\n",
    "            )\n",
    "            \n",
    "            if success:\n",
    "                # Convert rotation vector to rotation matrix.\n",
    "                rotation_matrix, _ = cv2.Rodrigues(rotation_vector)\n",
    "                # Decompose the rotation matrix using cv2.RQDecomp3x3.\n",
    "                # This returns six values: retval (Euler angles in degrees), mtxR, mtxQ, Qx, Qy, Qz.\n",
    "                retval, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rotation_matrix)\n",
    "\n",
    "                # Ensure retval is a flat array (if not, flatten it).\n",
    "                euler_angles = np.array(retval).flatten()\n",
    "\n",
    "                # Extract pitch, yaw, and roll from the Euler angles.\n",
    "                pitch = float(euler_angles[0])\n",
    "                yaw   = float(euler_angles[1])\n",
    "                roll  = float(euler_angles[2])\n",
    "\n",
    "                # Now you can compute differences relative to a baseline.\n",
    "                delta_pitch = abs(pitch - baseline_pitch)\n",
    "                delta_yaw   = abs(yaw - baseline_yaw)\n",
    "                delta_roll  = abs(roll - baseline_roll)\n",
    "\n",
    "                # Check if all Euler angles are within ±10° to determine if the head is facing forward.\n",
    "                if delta_pitch < 20 and delta_yaw < 20 and delta_roll < 20:\n",
    "                    status = \"Focused\"\n",
    "                else:\n",
    "                    status = \"Not Focused\"\n",
    "                    \n",
    "                # Display the computed angles on the frame.\n",
    "                cv2.putText(frame, f\"Pitch: {pitch:.1f}\", (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"Yaw: {yaw:.1f}\", (30, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"Roll: {roll:.1f}\", (30, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            else:\n",
    "                status = \"Pose Not Detected\"\n",
    "            \n",
    "            # Show the status (Focused/Not Focused) on the frame.\n",
    "            cv2.putText(frame, status, (30, 130), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0) if head_facing_forward else (0, 0, 255), 2)\n",
    "            \n",
    "            # Optionally, draw face landmarks on the frame.\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, face_landmarks, mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                landmark_drawing_spec=None, connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=1, circle_radius=1)\n",
    "            )\n",
    "    \n",
    "    cv2.imshow(\"Head Pose Estimation\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
